![main](img/1.png)
На последнем шаге лабораторной работы Docker Registry был запущен с подключённым внешним хранилищем (registry-data), смонтированным в /var/lib/registry.
После публикации образа hello-world в registry, в каталоге registry-data была сформирована стандартная структура Docker Registry v2 (docker/registry/v2/repositories, blobs), что подтверждает успешное сохранение образа вне контейнера registry.
![main](img/2.png)
![main](img/3.png)
![main](img/4.png)
Registry настроен на работу по протоколу HTTPS с использованием TLS-сертификата. Наличие HTTPS подтверждено выводом curl -vk (TLS handshake). Для доступа включена Basic Authentication через htpasswd. Без учётных данных сервер возвращает 401 Unauthorized. При передаче корректных учётных данных moby:gordon запрос к /v2/ завершается успешно с кодом 200 OK, что подтверждает успешную аутентификацию.
![main](img/5.png)
![main](img/6.png)
![main](img/7.png)
![main](img/8.png)
![main](img/9.png)
Вопрос 1: восстановилась ли работа запущенного сервиса на этом узле?
После перевода узла в Drain задачи сервиса были перенесены на другие узлы. После возврата узла в Active сервис продолжил работать, но задачи не обязаны автоматически возвращаться на этот узел.
Вопрос 2: что необходимо сделать, чтобы запустить работу службы на этом узле снова?
Нужно инициировать перепланирование задач, например выполнить docker service update --force <service> или изменить число реплик (docker service scale ...), чтобы Swarm создал новые задачи и разместил их на этом узле.
![main](img/10.png)
![main](img/11.png)
![main](img/12.png)
![main](img/13.png)
Как конфигурируется количество нодов в стэке
Количество нод не задаётся в файле стэка: оно определяется составом Swarm-кластера (команда docker node ls). В docker-stack.yml задаётся количество экземпляров сервисов через параметр deploy.replicas, а планировщик Docker Swarm распределяет реплики по доступным нодам автоматически.
Как организуется проверка жизнеспособности сервисов
Жизнеспособность обеспечивается механизмами Docker Swarm: сервис состоит из задач (tasks), и Swarm поддерживает требуемое число задач в состоянии Running. При сбое контейнера задача пересоздаётся согласно политикам в секции deploy (например restart_policy, update_config, если заданы). Дополнительно может использоваться healthcheck в compose/stack файле для определения состояния контейнера.
 
1. Кластеризованный вариант развертывания приложения

На основе репозитория со счётчиком реализован кластеризованный вариант развертывания приложения с использованием Docker Swarm.

Для развертывания используется файл docker-compose.swarm.yml и команда:

docker stack deploy -c docker-compose.swarm.yml counter

Масштабирование Flask-приложения

Количество инстансов контейнера, в котором работает Flask, явно задано и равно 4:

deploy:
  replicas: 4


Docker Swarm поддерживает декларативную модель состояния, поэтому оркестратор постоянно поддерживает наличие ровно четырёх реплик сервиса. В случае сбоя контейнер автоматически перезапускается (self-healing).

2. Подтверждение изменения потенциала обработки запросов (нагрузочное тестирование)

Для проверки изменения потенциала обработки запросов использовалось нагрузочное тестирование с помощью инструмента Locust (locustfile.py).

Тестируемые эндпоинты

GET /api/counter

POST /api/counter/increment

POST /api/counter/decrement

POST /api/counter/reset

Сравниваемые режимы

Одиночный запуск (docker-compose.yml)

1 инстанс Flask

1 инстанс Redis

Кластеризованный запуск (Swarm) (docker-compose.swarm.yml)

4 инстанса Flask

Redis с репликацией

Результаты (обобщённо)
Режим	Инстансы Flask	Потенциал обработки запросов
Одиночный запуск	1	Базовый
Docker Swarm	4	Выше / не линейно выше
Вывод

Потенциал обработки запросов изменился, так как:

web-приложение является stateless и эффективно масштабируется горизонтально;

нагрузка распределяется между 4 репликами через встроенный ingress-балансировщик Swarm.

При этом рост не является строго линейным из-за ограничений со стороны базы данных (stateful-компонент).

3. Увеличение количества инстансов сервера с БД

В качестве сервера БД используется Redis.

В кластеризованной конфигурации количество инстансов Redis увеличено до трёх:

1 master (redis-master);

2 replica (redis-replica-1, redis-replica-2).

Таким образом, требование «увеличить количество инстансов сервера с БД» выполнено.

4. Особенности работы реплицированного сервиса с БД

При работе с реплицированной базой данных существуют следующие особенности:

Запись выполняется только в master
Реплики Redis по умолчанию read-only, поэтому write-нагрузка не масштабируется.

Replication lag
Репликация асинхронная, возможна задержка между master и репликами, что приводит к eventual consistency.

Отсутствие автоматического failover
Без Redis Sentinel или Redis Cluster реплики не становятся master автоматически при отказе.

Пути решения

разделение потоков чтения и записи (read/write splitting);

использование Redis Sentinel для failover;

использование Redis Cluster для масштабирования записи.

В текущей реализации репликация повышает надёжность, но не увеличивает производительность базы данных.

5. Особенности depends_on в Docker Swarm

В Docker Swarm директива depends_on не обеспечивает порядок запуска и готовность сервисов. Все сервисы запускаются параллельно.

Корректная работа приложения обеспечивается за счёт:

retry-логики подключения к Redis в коде приложения;

политики перезапуска (restart_policy) в Docker Swarm.

6. Использование Kubernetes (k3s) вместо Docker Swarm

Альтернативный вариант кластеризации реализован с использованием Kubernetes (k3s) и приведён в файле k8s-deployment.yaml.

Отличия от Docker Swarm

масштабирование осуществляется через Deployment (replicas: 4);

балансировка нагрузки реализуется через Service;

stateful-сервисы требуют использования StatefulSet и PersistentVolume.

Kubernetes предоставляет более гибкие и развитые механизмы управления состоянием, однако Docker Swarm проще в настройке и эксплуатации.
